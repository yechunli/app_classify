{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba.analyse\n",
    "import collections\n",
    "from gensim import corpora, models\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "import operator\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = 'F:\\\\标注比赛\\\\大数据应用分类标注-选手\\\\apptype_train.dat'\n",
    "label_id = 'F:\\\\标注比赛\\\\大数据应用分类标注-选手\\\\apptype_id_name.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word = [] \n",
    "with open('F:\\标注比赛\\stop_words.txt') as f: \n",
    "    line = f.readline() \n",
    "    while line: \n",
    "        line = line.split('\\n') \n",
    "        stop_word.append(line[0]) \n",
    "        line = f.readline() \n",
    "    stop_word = set(stop_word)\n",
    "\n",
    "def data_process(data_file, model='test'): \n",
    "    enc = OneHotEncoder()\n",
    "    i = -1  \n",
    "    with open(data_file, encoding='utf-8') as f: \n",
    "        line = f.readline() \n",
    "        all_words = [] \n",
    "        sentenses = [] \n",
    "        app_id_list = [] \n",
    "        length = []\n",
    "        data = []\n",
    "        data_sentense = []\n",
    "        while line: \n",
    "            i = i + 1\n",
    "            #if i in youxi:\n",
    "            #    line = f.readline()\n",
    "            #    continue\n",
    "            desc_words = []\n",
    "            word_list = []\n",
    "            line = line.split('\\n')\n",
    "            line = line[0].split('\\t')\n",
    "            if model == 'train':   \n",
    "                two_id = line[1].split('|')\n",
    "                app_id = (int(two_id[0]) // 100)\n",
    "                #if app_id != 1401 and app_id != 1409:\n",
    "                #    line = f.readline()\n",
    "                #    continue\n",
    "                app_id_list.append(app_id)\n",
    "            app_desc = line[-1]\n",
    "            word_line = list(jieba.analyse.extract_tags(app_desc, withWeight=False, allowPOS=('n', 'vn')))\n",
    "            #word_line = list(jieba.cut(app_desc))\n",
    "            for word in word_line:\n",
    "                if word not in stop_word:#and word not in ['qingkan520','www','com','http']:\n",
    "                    desc_words.append(word)\n",
    "                    all_words.append(word)\n",
    "            #if app_id == 21:\n",
    "            #    print(line)\n",
    "            #    print(desc_words)\n",
    "            #    break\n",
    "            line = f.readline()\n",
    "\n",
    "            sentenses.append(desc_words)\n",
    "            \n",
    "            #i = i+ 1\n",
    "            #if i == 1000:\n",
    "            #    break\n",
    "#        counter = collections.Counter(all_words)\n",
    "#        word_dict = counter.most_common(30000)\n",
    "#        dict_list = [x[0] for x in word_dict]\n",
    "#        dictionary = {}\n",
    "#        for i in range(len(dict_list)):\n",
    "#            dictionary[dict_list[i]] = i\n",
    "\n",
    "        \n",
    "        num_doc_train = len(sentenses)\n",
    "        \n",
    "#        for queue in sentenses:\n",
    "#            for i in range(len(queue)):\n",
    "#                data_tmp = dictionary.get(queue[i])\n",
    "\n",
    "#                if not data_tmp:\n",
    "#                    continue\n",
    "#                data_sentense.append(data_tmp)\n",
    "#            length.append(len(data_sentense))\n",
    "#            data.append(data_sentense)\n",
    "#        data_len = np.mean(length)\n",
    "        \n",
    "#        for i in range(num_doc_train):\n",
    "#            if length[i] >= data_len:\n",
    "#                data[i] = data[i][:data_len]\n",
    "#            else:\n",
    "#                data[i] = data[i]+[0 for x in range(data_len-length[i])]\n",
    "#    label = enc.fit_transform(np.array(app_id_list).reshape(-1, 1)).toarray()\n",
    "#    return data, data_len, label\n",
    "        dictionary = corpora.Dictionary(sentenses)\n",
    "\n",
    "        dictionary.filter_extremes(no_below=100, no_above=0.2, keep_n=2000)\n",
    "        dictionary.compactify()\n",
    "\n",
    "        corpus = [dictionary.doc2bow(test) for test in sentenses]\n",
    "\n",
    "        tfidf = models.TfidfModel(corpus)\n",
    "        corpus_tfidf = tfidf[corpus]\n",
    "        return corpus, dictionary, num_doc_train, app_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDA_model(corpus_tfidf, dictionary, num_doc_train):\n",
    "    lda_model = models.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=2, chunksize=100, passes=2, iterations=5,\n",
    "                                minimum_probability=0.01, random_state=1)\n",
    "    topic = []\n",
    "    for i in range(num_doc_train):\n",
    "        topic_id = lda_model[corpus_tfidf[i]]\n",
    "        topic_id.sort(key=operator.itemgetter(1))\n",
    "        topic.append(topic_id[-1][0])\n",
    "    print(lda_model.print_topics())\n",
    "    print(lda_model.log_perplexity(corpus_tfidf))\n",
    "    return topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_compare(topic, app_id_list):\n",
    "    #print(sorted(app_id_list))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.015*\"查询\" + 0.011*\"生活\" + 0.010*\"APP\" + 0.009*\"实时\" + 0.009*\"平台\" + 0.008*\"定位\" + 0.008*\"通过\" + 0.008*\"出行\" + 0.007*\"新增\" + 0.007*\"及\"'), (1, '0.011*\"微信\" + 0.010*\"视频\" + 0.009*\"文件\" + 0.008*\"自动\" + 0.008*\"一键\" + 0.008*\"红包\" + 0.008*\"设置\" + 0.007*\"游戏\" + 0.007*\"下载\" + 0.007*\"安全\"')]\n",
      "-6.416588429290033\n"
     ]
    }
   ],
   "source": [
    "corpus_tfidf, dictionary, num_doc_train, app_id_list = data_process(data_file, 'train')\n",
    "topic = LDA_model(corpus_tfidf, dictionary, num_doc_train)\n",
    "label_compare(topic, app_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.array([topic,app_id_list]).transpose(), columns=['predict', 'label'])\n",
    "df_list = []\n",
    "for i in range(2):\n",
    "    df_list.append(df.loc[df.predict==i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1401    1502\n",
       "1409    1284\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_list[0]['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1409    3783\n",
       "1401     260\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_list[1]['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-78-6665138e4c9c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "df_list[2]['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-79-d21883faba22>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "df_list[3]['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-baeda4b5b9d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "df_list[4]['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1409    422\n",
       "1401    181\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_list[5]['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1409    5067\n",
       "1401    1762\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(np.array(app_id_list).transpose())[0].value_counts(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "youxi = []\n",
    "for i in range(len(app_id_list)):\n",
    "    if topic[i] == 2 and app_id_list[i] == 0:\n",
    "        youxi.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jiaoyu_and_licai = []\n",
    "for i in range(len(app_id_list)):\n",
    "    if topic[i] == 1:\n",
    "        jiaoyu_and_licai.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00000777CE5B5AA5C1AC94DB8EABE0AC\t140203\t《游戏王座》使用说明书成分由怪兽卡、魔法卡、陷阱卡合计数千张卡牌以及刺激性、耐久性玩法组成。性状本游戏为真正集换式卡牌游戏TCG，随着玩家的深入而不断释放独特魅力。功能主治锻炼思维，形成头脑风暴。对智商105以上者的智商有明显促进作用，对无脑游戏厌恶症、收集强迫症患者有治愈之疗效。不良反应经临床实验，对智商90以下者的智商和自信有降低和灭杀反应。注意事项使用本游戏时请务必仔细阅读游戏内的新手入门，同时请参考不良反应。试用后请自行判断自己是否适合继续使用。更新内容《游戏王座》使用说明书成分由怪兽卡、魔法卡、陷阱卡合计数千张卡牌以及刺激性、耐久性玩法组成。性状本游戏为真正集换式卡牌游戏TCG，随着玩家的深入而不断释放独特魅力。功能主治锻炼思维，形成头脑风暴。对智商105以上者的智商有明显促进作用，对无脑游戏厌恶症、收集强迫症患者有治愈之疗效。不良反应经临床实验，对智商90以下者的智商和自信有降低和灭杀反应。注意事项使用本游戏时请务必仔细阅读游戏内的新手入门，同时请参考不良反应。试用后请自行判断自己是否适合继续使用。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "with open( 'F:\\\\标注比赛\\\\大数据应用分类标注-选手\\\\apptype_train.dat', encoding='utf-8') as f:\n",
    "        line = f.readline()\n",
    "        while line:\n",
    "            if i in youxi:\n",
    "                print(line)\n",
    "                break\n",
    "            i = i + 1\n",
    "            line = f.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\python\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "e:\\python\\lib\\site-packages\\gensim\\interfaces.py:96: UserWarning: corpus.save() stores only the (tiny) iteration object in memory; to serialize the actual corpus content, use e.g. MmCorpus.serialize(corpus)\n",
      "  \"corpus.save() stores only the (tiny) iteration object in memory; \"\n",
      "e:\\python\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "dictionary.save('F:\\\\标注比赛\\\\大数据应用分类标注-选手\\\\ths_dict.dict')  # 保存生成的词典\n",
    "#dictionary = corpora.Dictionary.load('ths_dict.dict')  # 加载\n",
    "\n",
    "corpus_tfidf.save(\"F:\\\\标注比赛\\\\大数据应用分类标注-选手\\\\ths_tfidf.model\")#保存成model格式\n",
    "#corpus_tfidf = models.TfidfModel.load(\"ths_tfidf.model\")#加载\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LdaModel in module gensim.models.ldamodel:\n",
      "\n",
      "class LdaModel(gensim.interfaces.TransformationABC, gensim.models.basemodel.BaseTopicModel)\n",
      " |  Train and use Online Latent Dirichlet Allocation (OLDA) models as presented in\n",
      " |  `Hoffman et al. :\"Online Learning for Latent Dirichlet Allocation\" <https://www.di.ens.fr/~fbach/mdhnips2010.pdf>`_.\n",
      " |  \n",
      " |  Examples\n",
      " |  -------\n",
      " |  Initialize a model using a Gensim corpus\n",
      " |  \n",
      " |  .. sourcecode:: pycon\n",
      " |  \n",
      " |      >>> from gensim.test.utils import common_corpus\n",
      " |      >>>\n",
      " |      >>> lda = LdaModel(common_corpus, num_topics=10)\n",
      " |  \n",
      " |  You can then infer topic distributions on new, unseen documents.\n",
      " |  \n",
      " |  .. sourcecode:: pycon\n",
      " |  \n",
      " |      >>> doc_bow = [(1, 0.3), (2, 0.1), (0, 0.09)]\n",
      " |      >>> doc_lda = lda[doc_bow]\n",
      " |  \n",
      " |  The model can be updated (trained) with new documents.\n",
      " |  \n",
      " |  .. sourcecode:: pycon\n",
      " |  \n",
      " |      >>> # In practice (corpus =/= initial training corpus), but we use the same here for simplicity.\n",
      " |      >>> other_corpus = common_corpus\n",
      " |      >>>\n",
      " |      >>> lda.update(other_corpus)\n",
      " |  \n",
      " |  Model persistency is achieved through :meth:`~gensim.models.ldamodel.LdaModel.load` and\n",
      " |  :meth:`~gensim.models.ldamodel.LdaModel.save` methods.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LdaModel\n",
      " |      gensim.interfaces.TransformationABC\n",
      " |      gensim.utils.SaveLoad\n",
      " |      gensim.models.basemodel.BaseTopicModel\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, bow, eps=None)\n",
      " |      Get the topic distribution for the given document.\n",
      " |      \n",
      " |      Wraps :meth:`~gensim.models.ldamodel.LdaModel.get_document_topics` to support an operator style call.\n",
      " |      Uses the model's current state (set using constructor arguments) to fill in the additional arguments of the\n",
      " |      wrapper method.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ---------\n",
      " |      bow : list of (int, float)\n",
      " |          The document in BOW format.\n",
      " |      eps : float, optional\n",
      " |          Topics with an assigned probability lower than this threshold will be discarded.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (int, float)\n",
      " |          Topic distribution for the given document. Each topic is represented as a pair of its ID and the probability\n",
      " |          assigned to it.\n",
      " |  \n",
      " |  __init__(self, corpus=None, num_topics=100, id2word=None, distributed=False, chunksize=2000, passes=1, update_every=1, alpha='symmetric', eta=None, decay=0.5, offset=1.0, eval_every=10, iterations=50, gamma_threshold=0.001, minimum_probability=0.01, random_state=None, ns_conf=None, minimum_phi_value=0.01, per_word_topics=False, callbacks=None, dtype=<class 'numpy.float32'>)\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      corpus : {iterable of list of (int, float), scipy.sparse.csc}, optional\n",
      " |          Stream of document vectors or sparse matrix of shape (`num_terms`, `num_documents`).\n",
      " |          If not given, the model is left untrained (presumably because you want to call\n",
      " |          :meth:`~gensim.models.ldamodel.LdaModel.update` manually).\n",
      " |      num_topics : int, optional\n",
      " |          The number of requested latent topics to be extracted from the training corpus.\n",
      " |      id2word : {dict of (int, str), :class:`gensim.corpora.dictionary.Dictionary`}\n",
      " |          Mapping from word IDs to words. It is used to determine the vocabulary size, as well as for\n",
      " |          debugging and topic printing.\n",
      " |      distributed : bool, optional\n",
      " |          Whether distributed computing should be used to accelerate training.\n",
      " |      chunksize :  int, optional\n",
      " |          Number of documents to be used in each training chunk.\n",
      " |      passes : int, optional\n",
      " |          Number of passes through the corpus during training.\n",
      " |      update_every : int, optional\n",
      " |          Number of documents to be iterated through for each update.\n",
      " |          Set to 0 for batch learning, > 1 for online iterative learning.\n",
      " |      alpha : {numpy.ndarray, str}, optional\n",
      " |          Can be set to an 1D array of length equal to the number of expected topics that expresses\n",
      " |          our a-priori belief for the each topics' probability.\n",
      " |          Alternatively default prior selecting strategies can be employed by supplying a string:\n",
      " |      \n",
      " |              * 'asymmetric': Uses a fixed normalized asymmetric prior of `1.0 / topicno`.\n",
      " |              * 'auto': Learns an asymmetric prior from the corpus (not available if `distributed==True`).\n",
      " |      eta : {float, np.array, str}, optional\n",
      " |          A-priori belief on word probability, this can be:\n",
      " |      \n",
      " |              * scalar for a symmetric prior over topic/word probability,\n",
      " |              * vector of length num_words to denote an asymmetric user defined probability for each word,\n",
      " |              * matrix of shape (num_topics, num_words) to assign a probability for each word-topic combination,\n",
      " |              * the string 'auto' to learn the asymmetric prior from the data.\n",
      " |      decay : float, optional\n",
      " |          A number between (0.5, 1] to weight what percentage of the previous lambda value is forgotten\n",
      " |          when each new document is examined. Corresponds to Kappa from\n",
      " |          `Matthew D. Hoffman, David M. Blei, Francis Bach:\n",
      " |          \"Online Learning for Latent Dirichlet Allocation NIPS'10\" <https://www.di.ens.fr/~fbach/mdhnips2010.pdf>`_.\n",
      " |      offset : float, optional\n",
      " |          Hyper-parameter that controls how much we will slow down the first steps the first few iterations.\n",
      " |          Corresponds to Tau_0 from `Matthew D. Hoffman, David M. Blei, Francis Bach:\n",
      " |          \"Online Learning for Latent Dirichlet Allocation NIPS'10\" <https://www.di.ens.fr/~fbach/mdhnips2010.pdf>`_.\n",
      " |      eval_every : int, optional\n",
      " |          Log perplexity is estimated every that many updates. Setting this to one slows down training by ~2x.\n",
      " |      iterations : int, optional\n",
      " |          Maximum number of iterations through the corpus when inferring the topic distribution of a corpus.\n",
      " |      gamma_threshold : float, optional\n",
      " |          Minimum change in the value of the gamma parameters to continue iterating.\n",
      " |      minimum_probability : float, optional\n",
      " |          Topics with a probability lower than this threshold will be filtered out.\n",
      " |      random_state : {np.random.RandomState, int}, optional\n",
      " |          Either a randomState object or a seed to generate one. Useful for reproducibility.\n",
      " |      ns_conf : dict of (str, object), optional\n",
      " |          Key word parameters propagated to :func:`gensim.utils.getNS` to get a Pyro4 Nameserved.\n",
      " |          Only used if `distributed` is set to True.\n",
      " |      minimum_phi_value : float, optional\n",
      " |          if `per_word_topics` is True, this represents a lower bound on the term probabilities.\n",
      " |      per_word_topics : bool\n",
      " |          If True, the model also computes a list of topics, sorted in descending order of most likely topics for\n",
      " |          each word, along with their phi values multiplied by the feature length (i.e. word count).\n",
      " |      callbacks : list of :class:`~gensim.models.callbacks.Callback`\n",
      " |          Metric callbacks to log and visualize evaluation metrics of the model during training.\n",
      " |      dtype : {numpy.float16, numpy.float32, numpy.float64}, optional\n",
      " |          Data-type to use during calculations inside model. All inputs are also converted.\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Get a string representation of the current object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          Human readable representation of the most important model parameters.\n",
      " |  \n",
      " |  bound(self, corpus, gamma=None, subsample_ratio=1.0)\n",
      " |      Estimate the variational bound of documents from the corpus as E_q[log p(corpus)] - E_q[log q(corpus)].\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      corpus : {iterable of list of (int, float), scipy.sparse.csc}, optional\n",
      " |          Stream of document vectors or sparse matrix of shape (`num_terms`, `num_documents`) used to estimate the\n",
      " |          variational bounds.\n",
      " |      gamma : numpy.ndarray, optional\n",
      " |          Topic weight variational parameters for each document. If not supplied, it will be inferred from the model.\n",
      " |      subsample_ratio : float, optional\n",
      " |          Percentage of the whole corpus represented by the passed `corpus` argument (in case this was a sample).\n",
      " |          Set to 1.0 if the whole corpus was passed.This is used as a multiplicative factor to scale the likelihood\n",
      " |          appropriately.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          The variational bound score calculated for each document.\n",
      " |  \n",
      " |  clear(self)\n",
      " |      Clear the model's state to free some memory. Used in the distributed implementation.\n",
      " |  \n",
      " |  diff(self, other, distance='kullback_leibler', num_words=100, n_ann_terms=10, diagonal=False, annotation=True, normed=True)\n",
      " |      Calculate the difference in topic distributions between two models: `self` and `other`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`~gensim.models.ldamodel.LdaModel`\n",
      " |          The model which will be compared against the current object.\n",
      " |      distance : {'kullback_leibler', 'hellinger', 'jaccard', 'jensen_shannon'}\n",
      " |          The distance metric to calculate the difference with.\n",
      " |      num_words : int, optional\n",
      " |          The number of most relevant words used if `distance == 'jaccard'`. Also used for annotating topics.\n",
      " |      n_ann_terms : int, optional\n",
      " |          Max number of words in intersection/symmetric difference between topics. Used for annotation.\n",
      " |      diagonal : bool, optional\n",
      " |          Whether we need the difference between identical topics (the diagonal of the difference matrix).\n",
      " |      annotation : bool, optional\n",
      " |          Whether the intersection or difference of words between two topics should be returned.\n",
      " |      normed : bool, optional\n",
      " |          Whether the matrix should be normalized or not.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          A difference matrix. Each element corresponds to the difference between the two topics,\n",
      " |          shape (`self.num_topics`, `other.num_topics`)\n",
      " |      numpy.ndarray, optional\n",
      " |          Annotation matrix where for each pair we include the word from the intersection of the two topics,\n",
      " |          and the word from the symmetric difference of the two topics. Only included if `annotation == True`.\n",
      " |          Shape (`self.num_topics`, `other_model.num_topics`, 2).\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Get the differences between each pair of topics inferred by two models\n",
      " |      \n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.models.ldamulticore import LdaMulticore\n",
      " |          >>> from gensim.test.utils import datapath\n",
      " |          >>>\n",
      " |          >>> m1 = LdaMulticore.load(datapath(\"lda_3_0_1_model\"))\n",
      " |          >>> m2 = LdaMulticore.load(datapath(\"ldamodel_python_3_5\"))\n",
      " |          >>> mdiff, annotation = m1.diff(m2)\n",
      " |          >>> topic_diff = mdiff  # get matrix with difference for each topic pair from `m1` and `m2`\n",
      " |  \n",
      " |  do_estep(self, chunk, state=None)\n",
      " |      Perform inference on a chunk of documents, and accumulate the collected sufficient statistics.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      chunk : {list of list of (int, float), scipy.sparse.csc}\n",
      " |          The corpus chunk on which the inference step will be performed.\n",
      " |      state : :class:`~gensim.models.ldamodel.LdaState`, optional\n",
      " |          The state to be updated with the newly accumulated sufficient statistics. If none, the models\n",
      " |          `self.state` is updated.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Gamma parameters controlling the topic weights, shape (`len(chunk)`, `self.num_topics`).\n",
      " |  \n",
      " |  do_mstep(self, rho, other, extra_pass=False)\n",
      " |      Maximization step: use linear interpolation between the existing topics and\n",
      " |      collected sufficient statistics in `other` to update the topics.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      rho : float\n",
      " |          Learning rate.\n",
      " |      other : :class:`~gensim.models.ldamodel.LdaModel`\n",
      " |          The model whose sufficient statistics will be used to update the topics.\n",
      " |      extra_pass : bool, optional\n",
      " |          Whether this step required an additional pass over the corpus.\n",
      " |  \n",
      " |  get_document_topics(self, bow, minimum_probability=None, minimum_phi_value=None, per_word_topics=False)\n",
      " |      Get the topic distribution for the given document.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      bow : corpus : list of (int, float)\n",
      " |          The document in BOW format.\n",
      " |      minimum_probability : float\n",
      " |          Topics with an assigned probability lower than this threshold will be discarded.\n",
      " |      minimum_phi_value : float\n",
      " |          If `per_word_topics` is True, this represents a lower bound on the term probabilities that are included.\n",
      " |           If set to None, a value of 1e-8 is used to prevent 0s.\n",
      " |      per_word_topics : bool\n",
      " |          If True, this function will also return two extra lists as explained in the \"Returns\" section.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (int, float)\n",
      " |          Topic distribution for the whole document. Each element in the list is a pair of a topic's id, and\n",
      " |          the probability that was assigned to it.\n",
      " |      list of (int, list of (int, float), optional\n",
      " |          Most probable topics per word. Each element in the list is a pair of a word's id, and a list of\n",
      " |          topics sorted by their relevance to this word. Only returned if `per_word_topics` was set to True.\n",
      " |      list of (int, list of float), optional\n",
      " |          Phi relevance values, multiplied by the feature length, for each word-topic combination.\n",
      " |          Each element in the list is a pair of a word's id and a list of the phi values between this word and\n",
      " |          each topic. Only returned if `per_word_topics` was set to True.\n",
      " |  \n",
      " |  get_term_topics(self, word_id, minimum_probability=None)\n",
      " |      Get the most relevant topics to the given word.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word_id : int\n",
      " |          The word for which the topic distribution will be computed.\n",
      " |      minimum_probability : float, optional\n",
      " |          Topics with an assigned probability below this threshold will be discarded.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (int, float)\n",
      " |          The relevant topics represented as pairs of their ID and their assigned probability, sorted\n",
      " |          by relevance to the given word.\n",
      " |  \n",
      " |  get_topic_terms(self, topicid, topn=10)\n",
      " |      Get the representation for a single topic. Words the integer IDs, in constrast to\n",
      " |      :meth:`~gensim.models.ldamodel.LdaModel.show_topic` that represents words by the actual strings.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      topicid : int\n",
      " |          The ID of the topic to be returned\n",
      " |      topn : int, optional\n",
      " |          Number of the most significant words that are associated with the topic.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (int, float)\n",
      " |          Word ID - probability pairs for the most relevant words generated by the topic.\n",
      " |  \n",
      " |  get_topics(self)\n",
      " |      Get the term-topic matrix learned during inference.\n",
      " |      \n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          The probability for each word in each topic, shape (`num_topics`, `vocabulary_size`).\n",
      " |  \n",
      " |  inference(self, chunk, collect_sstats=False)\n",
      " |      Given a chunk of sparse document vectors, estimate gamma (parameters controlling the topic weights)\n",
      " |      for each document in the chunk.\n",
      " |      \n",
      " |      This function does not modify the model The whole input chunk of document is assumed to fit in RAM;\n",
      " |      chunking of a large corpus must be done earlier in the pipeline. Avoids computing the `phi` variational\n",
      " |      parameter directly using the optimization presented in\n",
      " |      `Lee, Seung: Algorithms for non-negative matrix factorization\"\n",
      " |      <https://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf>`_.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      chunk : {list of list of (int, float), scipy.sparse.csc}\n",
      " |          The corpus chunk on which the inference step will be performed.\n",
      " |      collect_sstats : bool, optional\n",
      " |          If set to True, also collect (and return) sufficient statistics needed to update the model's topic-word\n",
      " |          distributions.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      (numpy.ndarray, {numpy.ndarray, None})\n",
      " |          The first element is always returned and it corresponds to the states gamma matrix. The second element is\n",
      " |          only returned if `collect_sstats` == True and corresponds to the sufficient statistics for the M step.\n",
      " |  \n",
      " |  init_dir_prior(self, prior, name)\n",
      " |      Initialize priors for the Dirichlet distribution.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      prior : {str, list of float, numpy.ndarray of float, float}\n",
      " |          A-priori belief on word probability. If `name` == 'eta' then the prior can be:\n",
      " |      \n",
      " |              * scalar for a symmetric prior over topic/word probability,\n",
      " |              * vector of length num_words to denote an asymmetric user defined probability for each word,\n",
      " |              * matrix of shape (num_topics, num_words) to assign a probability for each word-topic combination,\n",
      " |              * the string 'auto' to learn the asymmetric prior from the data.\n",
      " |      \n",
      " |          If `name` == 'alpha', then the prior can be:\n",
      " |      \n",
      " |              * an 1D array of length equal to the number of expected topics,\n",
      " |              * 'asymmetric': Uses a fixed normalized asymmetric prior of `1.0 / topicno`.\n",
      " |              * 'auto': Learns an asymmetric prior from the corpus.\n",
      " |      name : {'alpha', 'eta'}\n",
      " |          Whether the `prior` is parameterized by the alpha vector (1 parameter per topic)\n",
      " |          or by the eta (1 parameter per unique term in the vocabulary).\n",
      " |  \n",
      " |  log_perplexity(self, chunk, total_docs=None)\n",
      " |      Calculate and return per-word likelihood bound, using a chunk of documents as evaluation corpus.\n",
      " |      \n",
      " |      Also output the calculated statistics, including the perplexity=2^(-bound), to log at INFO level.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      chunk : {list of list of (int, float), scipy.sparse.csc}\n",
      " |          The corpus chunk on which the inference step will be performed.\n",
      " |      total_docs : int, optional\n",
      " |          Number of docs used for evaluation of the perplexity.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          The variational bound score calculated for each word.\n",
      " |  \n",
      " |  save(self, fname, ignore=('state', 'dispatcher'), separately=None, *args, **kwargs)\n",
      " |      Save the model to a file.\n",
      " |      \n",
      " |      Large internal arrays may be stored into separate files, with `fname` as prefix.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      If you intend to use models across Python 2/3 versions there are a few things to\n",
      " |      keep in mind:\n",
      " |      \n",
      " |        1. The pickled Python dictionaries will not work across Python versions\n",
      " |        2. The `save` method does not automatically save all numpy arrays separately, only\n",
      " |           those ones that exceed `sep_limit` set in :meth:`~gensim.utils.SaveLoad.save`. The main\n",
      " |           concern here is the `alpha` array if for instance using `alpha='auto'`.\n",
      " |      \n",
      " |      Please refer to the `wiki recipes section\n",
      " |      <https://github.com/RaRe-Technologies/gensim/wiki/\n",
      " |      Recipes-&-FAQ#q9-how-do-i-load-a-model-in-python-3-that-was-trained-and-saved-using-python-2>`_\n",
      " |      for an example on how to work around these issues.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.ldamodel.LdaModel.load`\n",
      " |          Load model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the system file where the model will be persisted.\n",
      " |      ignore : tuple of str, optional\n",
      " |          The named attributes in the tuple will be left out of the pickled model. The reason why\n",
      " |          the internal `state` is ignored by default is that it uses its own serialisation rather than the one\n",
      " |          provided by this method.\n",
      " |      separately : {list of str, None}, optional\n",
      " |          If None -  automatically detect large numpy/scipy.sparse arrays in the object being stored, and store\n",
      " |          them into separate files. This avoids pickle memory errors and allows `mmap`'ing large arrays\n",
      " |          back on load efficiently. If list of str - this attributes will be stored in separate files,\n",
      " |          the automatic check is not performed in this case.\n",
      " |      *args\n",
      " |          Positional arguments propagated to :meth:`~gensim.utils.SaveLoad.save`.\n",
      " |      **kwargs\n",
      " |          Key word arguments propagated to :meth:`~gensim.utils.SaveLoad.save`.\n",
      " |  \n",
      " |  show_topic(self, topicid, topn=10)\n",
      " |      Get the representation for a single topic. Words here are the actual strings, in constrast to\n",
      " |      :meth:`~gensim.models.ldamodel.LdaModel.get_topic_terms` that represents words by their vocabulary ID.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      topicid : int\n",
      " |          The ID of the topic to be returned\n",
      " |      topn : int, optional\n",
      " |          Number of the most significant words that are associated with the topic.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float)\n",
      " |          Word - probability pairs for the most relevant words generated by the topic.\n",
      " |  \n",
      " |  show_topics(self, num_topics=10, num_words=10, log=False, formatted=True)\n",
      " |      Get a representation for selected topics.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      num_topics : int, optional\n",
      " |          Number of topics to be returned. Unlike LSA, there is no natural ordering between the topics in LDA.\n",
      " |          The returned topics subset of all topics is therefore arbitrary and may change between two LDA\n",
      " |          training runs.\n",
      " |      num_words : int, optional\n",
      " |          Number of words to be presented for each topic. These will be the most relevant words (assigned the highest\n",
      " |          probability for each topic).\n",
      " |      log : bool, optional\n",
      " |          Whether the output is also logged, besides being returned.\n",
      " |      formatted : bool, optional\n",
      " |          Whether the topic representations should be formatted as strings. If False, they are returned as\n",
      " |          2 tuples of (word, probability).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of {str, tuple of (str, float)}\n",
      " |          a list of topics, each represented either as a string (when `formatted` == True) or word-probability\n",
      " |          pairs.\n",
      " |  \n",
      " |  sync_state(self, current_Elogbeta=None)\n",
      " |      Propagate the states topic probabilities to the inner object's attribute.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      current_Elogbeta: numpy.ndarray\n",
      " |          Posterior probabilities for each topic, optional.\n",
      " |          If omitted, it will get Elogbeta from state.\n",
      " |  \n",
      " |  top_topics(self, corpus=None, texts=None, dictionary=None, window_size=None, coherence='u_mass', topn=20, processes=-1)\n",
      " |      Get the topics with the highest coherence score the coherence for each topic.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      corpus : iterable of list of (int, float), optional\n",
      " |          Corpus in BoW format.\n",
      " |      texts : list of list of str, optional\n",
      " |          Tokenized texts, needed for coherence models that use sliding window based (i.e. coherence=`c_something`)\n",
      " |          probability estimator .\n",
      " |      dictionary : :class:`~gensim.corpora.dictionary.Dictionary`, optional\n",
      " |          Gensim dictionary mapping of id word to create corpus.\n",
      " |          If `model.id2word` is present, this is not needed. If both are provided, passed `dictionary` will be used.\n",
      " |      window_size : int, optional\n",
      " |          Is the size of the window to be used for coherence measures using boolean sliding window as their\n",
      " |          probability estimator. For 'u_mass' this doesn't matter.\n",
      " |          If None - the default window sizes are used which are: 'c_v' - 110, 'c_uci' - 10, 'c_npmi' - 10.\n",
      " |      coherence : {'u_mass', 'c_v', 'c_uci', 'c_npmi'}, optional\n",
      " |          Coherence measure to be used.\n",
      " |          Fastest method - 'u_mass', 'c_uci' also known as `c_pmi`.\n",
      " |          For 'u_mass' corpus should be provided, if texts is provided, it will be converted to corpus\n",
      " |          using the dictionary. For 'c_v', 'c_uci' and 'c_npmi' `texts` should be provided (`corpus` isn't needed)\n",
      " |      topn : int, optional\n",
      " |          Integer corresponding to the number of top words to be extracted from each topic.\n",
      " |      processes : int, optional\n",
      " |          Number of processes to use for probability estimation phase, any value less than 1 will be interpreted as\n",
      " |          num_cpus - 1.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (list of (int, str), float)\n",
      " |          Each element in the list is a pair of a topic representation and its coherence score. Topic representations\n",
      " |          are distributions of words, represented as a list of pairs of word IDs and their probabilities.\n",
      " |  \n",
      " |  update(self, corpus, chunksize=None, decay=None, offset=None, passes=None, update_every=None, eval_every=None, iterations=None, gamma_threshold=None, chunks_as_numpy=False)\n",
      " |      Train the model with new documents, by EM-iterating over the corpus until the topics converge, or until\n",
      " |      the maximum number of allowed iterations is reached. `corpus` must be an iterable.\n",
      " |      \n",
      " |      In distributed mode, the E step is distributed over a cluster of machines.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This update also supports updating an already trained model with new documents; the two models are then merged\n",
      " |      in proportion to the number of old vs. new documents. This feature is still experimental for non-stationary\n",
      " |      input streams. For stationary input (no topic drift in new documents), on the other hand, this equals the\n",
      " |      online update of `Matthew D. Hoffman, David M. Blei, Francis Bach:\n",
      " |      \"Online Learning for Latent Dirichlet Allocation NIPS'10\" <https://www.di.ens.fr/~fbach/mdhnips2010.pdf>`_.\n",
      " |      and is guaranteed to converge for any `decay` in (0.5, 1.0). Additionally, for smaller corpus sizes, an\n",
      " |      increasing `offset` may be beneficial (see Table 1 in the same paper).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      corpus : {iterable of list of (int, float), scipy.sparse.csc}, optional\n",
      " |          Stream of document vectors or sparse matrix of shape (`num_terms`, `num_documents`) used to update the\n",
      " |          model.\n",
      " |      chunksize :  int, optional\n",
      " |          Number of documents to be used in each training chunk.\n",
      " |      decay : float, optional\n",
      " |          A number between (0.5, 1] to weight what percentage of the previous lambda value is forgotten\n",
      " |          when each new document is examined. Corresponds to Kappa from\n",
      " |          `Matthew D. Hoffman, David M. Blei, Francis Bach:\n",
      " |          \"Online Learning for Latent Dirichlet Allocation NIPS'10\" <https://www.di.ens.fr/~fbach/mdhnips2010.pdf>`_.\n",
      " |      offset : float, optional\n",
      " |          Hyper-parameter that controls how much we will slow down the first steps the first few iterations.\n",
      " |          Corresponds to Tau_0 from `Matthew D. Hoffman, David M. Blei, Francis Bach:\n",
      " |          \"Online Learning for Latent Dirichlet Allocation NIPS'10\" <https://www.di.ens.fr/~fbach/mdhnips2010.pdf>`_.\n",
      " |      passes : int, optional\n",
      " |          Number of passes through the corpus during training.\n",
      " |      update_every : int, optional\n",
      " |          Number of documents to be iterated through for each update.\n",
      " |          Set to 0 for batch learning, > 1 for online iterative learning.\n",
      " |      eval_every : int, optional\n",
      " |          Log perplexity is estimated every that many updates. Setting this to one slows down training by ~2x.\n",
      " |      iterations : int, optional\n",
      " |          Maximum number of iterations through the corpus when inferring the topic distribution of a corpus.\n",
      " |      gamma_threshold : float, optional\n",
      " |          Minimum change in the value of the gamma parameters to continue iterating.\n",
      " |      chunks_as_numpy : bool, optional\n",
      " |          Whether each chunk passed to the inference step should be a numpy.ndarray or not. Numpy can in some settings\n",
      " |          turn the term IDs into floats, these will be converted back into integers in inference, which incurs a\n",
      " |          performance hit. For distributed computing it may be desirable to keep the chunks as `numpy.ndarray`.\n",
      " |  \n",
      " |  update_alpha(self, gammat, rho)\n",
      " |      Update parameters for the Dirichlet prior on the per-document topic weights.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      gammat : numpy.ndarray\n",
      " |          Previous topic weight parameters.\n",
      " |      rho : float\n",
      " |          Learning rate.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Sequence of alpha parameters.\n",
      " |  \n",
      " |  update_eta(self, lambdat, rho)\n",
      " |      Update parameters for the Dirichlet prior on the per-topic word weights.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      lambdat : numpy.ndarray\n",
      " |          Previous lambda parameters.\n",
      " |      rho : float\n",
      " |          Learning rate.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          The updated eta parameters.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load(fname, *args, **kwargs) from builtins.type\n",
      " |      Load a previously saved :class:`gensim.models.ldamodel.LdaModel` from file.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.ldamodel.LdaModel.save`\n",
      " |          Save model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the file where the model is stored.\n",
      " |      *args\n",
      " |          Positional arguments propagated to :meth:`~gensim.utils.SaveLoad.load`.\n",
      " |      **kwargs\n",
      " |          Key word arguments propagated to :meth:`~gensim.utils.SaveLoad.load`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Large arrays can be memmap'ed back as read-only (shared memory) by setting `mmap='r'`:\n",
      " |      \n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.test.utils import datapath\n",
      " |          >>>\n",
      " |          >>> fname = datapath(\"lda_3_0_1_model\")\n",
      " |          >>> lda = LdaModel.load(fname, mmap='r')\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.models.basemodel.BaseTopicModel:\n",
      " |  \n",
      " |  print_topic(self, topicno, topn=10)\n",
      " |      Get a single topic as a formatted string.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      topicno : int\n",
      " |          Topic id.\n",
      " |      topn : int\n",
      " |          Number of words from topic that will be used.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          String representation of topic, like '-0.340 * \"category\" + 0.298 * \"$M$\" + 0.183 * \"algebra\" + ... '.\n",
      " |  \n",
      " |  print_topics(self, num_topics=20, num_words=10)\n",
      " |      Get the most significant topics (alias for `show_topics()` method).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      num_topics : int, optional\n",
      " |          The number of topics to be selected, if -1 - all topics will be in result (ordered by significance).\n",
      " |      num_words : int, optional\n",
      " |          The number of words to be included per topics (ordered by significance).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (int, list of (str, float))\n",
      " |          Sequence with (topic_id, [(word, value), ... ]).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(models.LdaModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = [1,2,3,4,5]\n",
    "b[:-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
